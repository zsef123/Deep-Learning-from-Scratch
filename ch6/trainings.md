Chapter 6. 학습 관련 기술들
=====================

## 매개 변수 갱신
신경망 학습의 목적은 손실 함수의 값을 가능한 한 낮추는 매개 변수를 찾는 것.

1. SGD

미분 값을 이용해 최적의 값을 찾는다.
Convex한 형태에서는 값을 잘 찾지만
Local minimum != Global mininum 일 수가 있다.

또한 비등방성Anisotropy 함수에서 탐색 경로가 비효율적이다.

2. Momentum

운동량을 뜻하는 단어.
기울기 방향으로 힘을 받아 물체가 가속되듯이 최적의 값을 찾아 나간다.
비등방성 함수에서 보다 잘 찾게 되며
Local minimum에서 벗어 날 수 있다.

3. AdaGrad

학습을 진행하며 학습률Learning rate 를 변경시킨다.
개별 매개변수에 적응적으로Adaptive 학습률을 조정한다.

4. Adam

모멘텀 + AdaGrad 방식.
*자세한 부분은 원논문을 참조하라고 되어있다.

[optimizer.py](optimizer.py)

## 가중치의 초기 값

가중치 감소Weight Decay 기법에선 가중치 값을 작게 하여 오버피팅을 방지한다.
초기값 또한 최대한 작은 값에서 시작을 한다.
이때 초기 값이 0 일시 역전파에서 모든 가중치의 값이 똑같이 갱신 되므로 학습이 이루어 지지 않는다.

+ 기울기 소실Gradient Vanishing - 층이 깊은 딥러닝 에서 역전파의 기울기 값이 점점 작아지다가 사라진다

+ 또한 Layer 별 활성화값 분포가 치우쳤다는 것은 다수의 뉴런이 동일한 동작을 한다는 것 - 표현력 제한

1. Xavier 초기값

일반적인 딥러닝 프레임워크에서 표준적으로 이용한다. ex) Caffe
초기 값의 표준 편차가 1/root(N) 이 되는 분포를 사용한다.

활성화가 선형인 것을 전제로 이끈 결과이다. 

2. He 초기 값

ReLU에 특화된 초기 값.
표준편차가 root( 2/N )인 정규 분포를 사용한다.

## 배치 정규화Batch Normalization

활성화 값 분포가 적당히 퍼지도록 가중치 초기 값을 적절히 설정해야한다.
이 때 각 층의 활성화를 적당히 퍼뜨리도록 강제한다.

학습 시 미니 배치단위로 정규화 수행.( 활성화 함수 앞, 뒤 등에서 수행한다.)

## 오버 피팅Overfitting

훈련 데이터에 지나치게 적응되는 문제.
매개 변수가 많고 표현력이 높은 모델, 훈련 데이터가 적을 시 나타난다.
범용 데이터에선 오차률이 커진다.

오버 피팅 억제용
1. 가중치 감소Weight decay 
2. 드롭아웃
뉴런을 임의로 삭제하면서 학습하는 방법.
훈련시 은닉층의 뉴런을 무작위로 삭제.

## 하이퍼파라미터

각 층의 뉴런 수, 배치 크기, Learning rate 등 적절히 설정해야하는 값

훈련 데이터, 시험데이터, 검증 데이터로 나눈다.
훈련 데이터 - 매개변수 학습
시험 데이터 - 신경망의 범용 성능 평가
검증 데이터 - 하이퍼파라미터 성능 평가

최적화의 핵심은 하이퍼 파라미터의 최적 값이 존재하는 범위를 조금 씩 줄여가는것.
범위 설정 - Sampling - 정확도 평가



